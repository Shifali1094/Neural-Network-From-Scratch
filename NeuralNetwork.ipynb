{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03fb2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f862a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(layer_sizes, activation='relu', seed=42):\n",
    "    np.random.seed(seed)\n",
    "    num_layers = len(layer_sizes) - 1\n",
    "\n",
    "    # Initialize weights and biases using Xavier initialisation\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        weight_matrix = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / (layer_sizes[i] + layer_sizes[i+1]))\n",
    "        bias_vector = np.zeros((1, layer_sizes[i+1]))\n",
    "        weights.append(weight_matrix)\n",
    "        biases.append(bias_vector)\n",
    "    \n",
    "    print(f\"Initialized network with layer sizes: {layer_sizes}\")\n",
    "    print(f\"Activation function: {activation}\")\n",
    "    print(f\"Number of layers: {num_layers}\")\n",
    "    return weights, biases, layer_sizes, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e007713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(weights, biases):\n",
    "    total_params = 0\n",
    "    for w, b in zip(weights, biases):\n",
    "        total_params += w.size + b.size\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89536767",
   "metadata": {},
   "source": [
    "Activation functiosn with their respective derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ab5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def apply_activation(x, activation_type):\n",
    "    if activation_type == 'relu':\n",
    "        return relu(x)\n",
    "    elif activation_type == 'tanh':\n",
    "        return tanh(x)\n",
    "    elif activation_type == 'sigmoid':\n",
    "        return sigmoid(x)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function: {activation_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41e4d8",
   "metadata": {},
   "source": [
    "Forward propagation throughout the network\n",
    "\n",
    "mathematical formulation: \n",
    "    For each layer *l*: \n",
    "    Z[l] = A[l-1] @ W[l] + b[l] (Linear transformation)\n",
    "    A[l] = activation(Z[l]) (Activation Function)\n",
    "\n",
    "    Final Layer uses Softmax for classifcation\n",
    "\n",
    "    Args: \n",
    "        X: Input data (batch_size, input_features)\n",
    "        return_cache: If true return immediate values for back prop\n",
    "\n",
    "    returns: \n",
    "        Predictions: Final output probaility (batch_size, num_classes)\n",
    "        cache: Dictionary of intermediate values (if return_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e12257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, weights, biases, activation_type, return_cache=False):\n",
    "    cache = {}\n",
    "    A = X\n",
    "    cache['A0'] = X # Store input as activation of layer 0\n",
    "\n",
    "    num_layers = len(weights)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        # Hidden Layers\n",
    "        Z = np.dot(A, weights[i]) + biases[i]\n",
    "        \n",
    "        if i < num_layers - 1:\n",
    "            A = apply_activation(Z, activation_type)\n",
    "        else:\n",
    "            A = softmax(Z)  # Output layer with softmax\n",
    "        \n",
    "        cache[f'Z{i+1}'] = Z\n",
    "        cache[f'A{i+1}'] = A\n",
    "\n",
    "    if return_cache:\n",
    "        return A, cache\n",
    "    return A\n",
    "\n",
    "def predict(X, weights, biases, activation_type):\n",
    "    probabilities = forward_pass(X, return_cache=False)\n",
    "    return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6712c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_forward_pass():\n",
    "    print(\"=\"*50)\n",
    "    print(\"Testing forward pass\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "    y = np.array([0,1,1,0], dtype=int)  # XOR truth table\n",
    "\n",
    "    weights, biases, layer_sizes, activation = initialise(layer_sizes=[2, 2, 2], activation='relu')\n",
    "\n",
    "    print(\"\\nNetwork Architecture:\")\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        print(f\" Layer {i+1}: {layer_sizes[i]} -> {layer_sizes[i+1]}\")\n",
    "\n",
    "    # forward pass\n",
    "    print(f\"\\n{'-'*30}\")\n",
    "    print(\"FORWARD PASS:\")\n",
    "    print(f\"{'-'*30}\")\n",
    "\n",
    "    predictions, cache = forward_pass(X, weights, biases, activation, return_cache=True)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\" Predicted Probabilities:\\n{predictions}\")\n",
    "    print(f\" Predicted Classes: {predicted_classes}\")\n",
    "    print(f\" True Classes: {y}\")\n",
    "    print(f\"Accuracy: {np.mean(predicted_classes == y) * 100:.2f}%\")\n",
    "\n",
    "    return weights, biases, layer_sizes, activation, X, y, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738964e1",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1aa7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\n",
    "    '''\n",
    "    Compute the softmax activation for a vector or a matrix of logits.\n",
    "\n",
    "    Args: \n",
    "        x: A numpy array of shape (n,) or (m, n) where n is the number of classes.\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x representing the softmax probabilities.\n",
    "    '''\n",
    "\n",
    "    shifted_x = x - np.max(x, axis=-1, keepdims=True)  # Prevent overflow\n",
    "    exp_x = np.exp(shifted_x)\n",
    "\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c08b9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "\n",
    "    '''\n",
    "    Compute the cross-entropy loss between true labels and predicted probabilities.\n",
    "    Mathematical Formula:\n",
    "    L = -1/N * Σ(i=1 to N) Σ(j=1 to C) y_true[i,j] * log(y_pred[i,j])\n",
    "\n",
    "    Args:\n",
    "        y_true: A numpy array of shape (m, n) representing one-hot encoded true labels.\n",
    "        y_pred: A numpy array of shape (m, n) representing predicted probabilities.\n",
    "    Returns:\n",
    "        A float representing the average cross-entropy loss over the batch.\n",
    "    '''\n",
    "    batch_size = y_pred.shape[0]\n",
    "    num_classes = y_pred.shape[1]\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        y_true_onehot = np.zeros((batch_size, num_classes))\n",
    "        y_true_onehot[np.arange(batch_size), y_true] = 1\n",
    "    else:\n",
    "        y_true_onehot = y_true\n",
    "\n",
    "    epsilon = 1e-15\n",
    "    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Calclulate cross-entropy loss\n",
    "    sample_losses = -np.sum(y_true_onehot * np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "    return np.mean(sample_losses), y_true_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d963fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(cache, y_true_onehot, weights, activation_type):\n",
    "    \"\"\"\n",
    "    Backward propagation through the network\n",
    "    \n",
    "    Mathematical Formulation:\n",
    "    \n",
    "    For output layer (L):\n",
    "    dZ[L] = A[L] - y_true  (gradient after softmax + cross-entropy)\n",
    "    dW[L] = (1/m) * A[L-1].T @ dZ[L]\n",
    "    db[L] = (1/m) * sum(dZ[L], axis=0)\n",
    "    \n",
    "    For hidden layers (l = L-1, L-2, ..., 1):\n",
    "    dA[l] = dZ[l+1] @ W[l+1].T\n",
    "    dZ[l] = dA[l] * activation_derivative(Z[l])\n",
    "    dW[l] = (1/m) * A[l-1].T @ dZ[l]\n",
    "    db[l] = (1/m) * sum(dZ[l], axis=0)\n",
    "    \n",
    "    Args:\n",
    "        cache: Dictionary containing forward pass intermediate values\n",
    "        y_true_onehot: One-hot encoded true labels (batch_size, num_classes)\n",
    "        weights: List of weight matrices\n",
    "        activation_type: Type of activation function used in hidden layers\n",
    "    \n",
    "    Returns:\n",
    "        gradients: Dictionary containing gradients for weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    num_layers = len(weights)\n",
    "    batch_size = y_true_onehot.shape[0]\n",
    "    \n",
    "    # Initialize gradients lists with correct size\n",
    "    dW_gradients = [None] * num_layers\n",
    "    db_gradients = [None] * num_layers\n",
    "    \n",
    "    print(f\"Starting backpropagation...\")\n",
    "    print(f\"Batch size: {batch_size}, Number of layers: {num_layers}\")\n",
    "    \n",
    "    # === OUTPUT LAYER GRADIENTS ===\n",
    "    # For softmax + cross-entropy, the gradient simplifies to: dZ = A - y_true\n",
    "    A_output = cache[f'A{num_layers}']  # Final predictions (after softmax)\n",
    "    dZ = A_output - y_true_onehot  # Shape: (batch_size, num_classes)\n",
    "    \n",
    "    print(f\"Output layer gradient dZ shape: {dZ.shape}\")\n",
    "    \n",
    "    # Gradients for output layer weights and biases (last layer index)\n",
    "    A_prev = cache[f'A{num_layers-1}']  # Activations from previous layer\n",
    "    dW = (1/batch_size) * A_prev.T @ dZ  # Shape: (prev_layer_size, num_classes)\n",
    "    db = (1/batch_size) * np.sum(dZ, axis=0, keepdims=True)  # Shape: (1, num_classes)\n",
    "    \n",
    "    # Store output layer gradients at correct index\n",
    "    dW_gradients[num_layers-1] = dW\n",
    "    db_gradients[num_layers-1] = db\n",
    "    \n",
    "    print(f\"Output layer (index {num_layers-1}): dW shape = {dW.shape}, db shape = {db.shape}\")\n",
    "    \n",
    "    # === HIDDEN LAYER GRADIENTS ===\n",
    "    # Work backwards from layer (L-1) to layer 0\n",
    "    for layer_idx in range(num_layers - 2, -1, -1):  # num_layers-2, num_layers-3, ..., 0\n",
    "        # Propagate error backwards: dA = dZ_next @ W_next.T\n",
    "        next_layer_idx = layer_idx + 1\n",
    "        dA = dZ @ weights[next_layer_idx].T  # Shape: (batch_size, current_layer_size)\n",
    "        \n",
    "        # Get pre-activation values and compute activation derivative\n",
    "        Z_current = cache[f'Z{layer_idx+1}']  # Pre-activation values (cache uses 1-based indexing)\n",
    "        activation_derivative = apply_activation(Z_current, activation_type)\n",
    "        \n",
    "        # Apply chain rule: dZ = dA * activation_derivative\n",
    "        dZ = dA * activation_derivative  # Element-wise multiplication\n",
    "        \n",
    "        # Get activations from previous layer\n",
    "        A_prev = cache[f'A{layer_idx}']  # Previous layer activations\n",
    "        \n",
    "        # Calculate weight and bias gradients\n",
    "        dW = (1/batch_size) * A_prev.T @ dZ\n",
    "        db = (1/batch_size) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        \n",
    "        # Store gradients at correct index\n",
    "        dW_gradients[layer_idx] = dW\n",
    "        db_gradients[layer_idx] = db\n",
    "        \n",
    "        print(f\"Layer {layer_idx}: dZ shape = {dZ.shape}, dW shape = {dW.shape}, db shape = {db.shape}\")\n",
    "    \n",
    "    gradients = {'dW': dW_gradients, 'db': db_gradients}\n",
    "    \n",
    "    print(\"Backpropagation complete!\")\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "122433f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, biases, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update weights and biases using gradients\n",
    "    \n",
    "    Mathematical Update Rule:\n",
    "    W_new = W_old - learning_rate * dW\n",
    "    b_new = b_old - learning_rate * db\n",
    "    \n",
    "    Args:\n",
    "        weights: List of current weight matrices\n",
    "        biases: List of current bias vectors\n",
    "        gradients: Dictionary containing gradients\n",
    "        learning_rate: Step size for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "        Updated weights and biases (in-place modification)\n",
    "    \"\"\"\n",
    "    num_layers = len(weights)\n",
    "    for i in range(num_layers):\n",
    "        weights[i] -= learning_rate * gradients['dW'][i]\n",
    "        biases[i] -= learning_rate * gradients['db'][i]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be9e3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(X, y_true, weights, biases, activation_type, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Complete training step: forward pass + backward pass + weight update\n",
    "    \n",
    "    Args:\n",
    "        X: Input data (batch_size, input_features)\n",
    "        y_true: True labels (batch_size,)\n",
    "        weights: List of weight matrices\n",
    "        biases: List of bias vectors\n",
    "        activation_type: Activation function for hidden layers\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "        loss: Current loss value\n",
    "        accuracy: Current accuracy percentage\n",
    "        weights: Updated weights\n",
    "        biases: Updated biases\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    predictions, cache = forward_pass(X, weights, biases, activation_type, return_cache=True)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss, y_true_onehot = cross_entropy_loss(y_true, predictions)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_classes == y_true) * 100\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = backward_pass(cache, y_true_onehot, weights, activation_type)\n",
    "    \n",
    "    # Update weights\n",
    "    weights, biases = update_weights(weights, biases, gradients, learning_rate)\n",
    "    \n",
    "    return loss, accuracy, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a97f5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_backpropagation_xor():\n",
    "    \"\"\"Test backpropagation on XOR problem\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING BACKPROPAGATION ON XOR PROBLEM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # XOR dataset\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1], \n",
    "                  [1, 0],\n",
    "                  [1, 1]], dtype=np.float32)\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "    \n",
    "    print(f\"XOR Dataset:\")\n",
    "    print(f\"X:\\n{X}\")\n",
    "    print(f\"y: {y}\")\n",
    "    \n",
    "    # Initialize network\n",
    "    weights, biases, layer_sizes, activation = initialise([2, 4, 2], 'relu', seed=42)\n",
    "    \n",
    "    print(f\"\\nInitial network architecture: {layer_sizes}\")\n",
    "    print(f\"Activation: {activation}\")\n",
    "    \n",
    "    # Train for a few steps\n",
    "    learning_rate = 0.1\n",
    "    num_steps = 10\n",
    "    \n",
    "    print(f\"\\nTraining for {num_steps} steps with learning_rate = {learning_rate}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        loss, accuracy, weights, biases = train_one_step(\n",
    "            X, y, weights, biases, activation, learning_rate\n",
    "        )\n",
    "        \n",
    "        if step % 2 == 0 or step == num_steps - 1:  # Print every 2nd step\n",
    "            print(f\"Step {step+1:2d}: Loss = {loss:.6f}, Accuracy = {accuracy:5.1f}%\")\n",
    "    \n",
    "    # Final test\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    final_predictions = forward_pass(X, weights, biases, activation)\n",
    "    predicted_classes = np.argmax(final_predictions, axis=1)\n",
    "    final_accuracy = np.mean(predicted_classes == y) * 100\n",
    "    \n",
    "    print(f\"Final predictions:\\n{final_predictions}\")\n",
    "    print(f\"Predicted classes: {predicted_classes}\")\n",
    "    print(f\"True classes:      {y}\")\n",
    "    print(f\"Final accuracy: {final_accuracy:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nImprovement: Network should be learning to solve XOR!\")\n",
    "    if final_accuracy > 75:\n",
    "        print(\"The network is learning\")\n",
    "    else:\n",
    "        print(\"Try more training steps or different learning rate\")\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13c7ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING BACKPROPAGATION ON XOR PROBLEM\n",
      "============================================================\n",
      "XOR Dataset:\n",
      "X:\n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "y: [0 1 1 0]\n",
      "Initialized network with layer sizes: [2, 4, 2]\n",
      "Activation function: relu\n",
      "Number of layers: 2\n",
      "\n",
      "Initial network architecture: [2, 4, 2]\n",
      "Activation: relu\n",
      "\n",
      "Training for 10 steps with learning_rate = 0.1\n",
      "------------------------------------------------------------\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Step  1: Loss = 0.719186, Accuracy =  75.0%\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Step  3: Loss = 0.708744, Accuracy =  50.0%\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Step  5: Loss = 0.702261, Accuracy =  50.0%\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Step  7: Loss = 0.697929, Accuracy =  50.0%\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Step  9: Loss = 0.694831, Accuracy =  50.0%\n",
      "Starting backpropagation...\n",
      "Batch size: 4, Number of layers: 2\n",
      "Output layer gradient dZ shape: (4, 2)\n",
      "Output layer (index 1): dW shape = (4, 2), db shape = (1, 2)\n",
      "Layer 0: dZ shape = (4, 4), dW shape = (2, 4), db shape = (1, 4)\n",
      "Backpropagation complete!\n",
      "Step 10: Loss = 0.693584, Accuracy =  50.0%\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Final predictions:\n",
      "[[0.47446269 0.52553731]\n",
      " [0.6309916  0.3690084 ]\n",
      " [0.38966396 0.61033604]\n",
      " [0.58644297 0.41355703]]\n",
      "Predicted classes: [1 0 1 0]\n",
      "True classes:      [0 1 1 0]\n",
      "Final accuracy: 50.0%\n",
      "\n",
      "Improvement: Network should be learning to solve XOR!\n",
      "Try more training steps or different learning rate\n",
      "\n",
      "============================================================\n",
      "BACKPROPAGATION IMPLEMENTATION COMPLETE!\n",
      "============================================================\n",
      "Key functions implemented:\n",
      "- backward_pass(): Calculates gradients\n",
      "- update_weights(): Updates parameters using gradients\n",
      "- train_one_step(): Complete training iteration\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trained_weights, trained_biases = test_backpropagation_xor()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BACKPROPAGATION IMPLEMENTATION COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Key functions implemented:\")\n",
    "    print(\"- backward_pass(): Calculates gradients\")\n",
    "    print(\"- update_weights(): Updates parameters using gradients\") \n",
    "    print(\"- train_one_step(): Complete training iteration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
